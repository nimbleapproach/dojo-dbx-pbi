# Databricks Jobs Configuration.
# This file defines the jobs and their configurations for deployment to Databricks.
# Each job can represent a notebook, library, or other executable entity within Databricks.
## for additional options please see schema.json in that repository (can be generated by "databricks bundle schema" command)
resources:
  jobs:
    foundations_v1_job_crm_monthly_ghs: # Provide a unique name for this bundle. This name should differentiate it from other projects.
      name: foundations_v1_job_crm_monthly_ghs${var.suffix} # Provide a unique name for this job. This name should differentiate it from other projects.
      job_clusters:
      # Configuration for the new cluster that will be spun up for this job.
      - job_cluster_key: foundations_v1_job_crm_monthly_ghs_cluster${var.suffix} # Provide a unique name for this cluster. This name should differentiate it from other projects.
        new_cluster: 
          num_workers: 8 # Number of worker nodes.
          driver_node_type_id: Standard_D4ds_v5 # use general type always for driver, monitor CPU and RAM usage of driver to adjust driver type, try to choose new generation of VMs
          node_type_id: Standard_D16ds_v5  # use compute optimized for ML tasks (Standard_F4s_v2, Standard_F8s_v2, Standard_F16s_v2), use general for data engineering, try to choose new generation of VMs
          spark_version: "14.3.x-photon-scala2.12"  # use ML LTS for ML tasks, use Photon LTS for data engineering
          azure_attributes:
            availability: "SPOT_WITH_FALLBACK_AZURE"
            spot_bid_max_price: -1
      max_concurrent_runs: 1
      timeout_seconds: 14400
      email_notifications:
        on_failure:
          - ${var.email_team_notification}
          - ${var.email_mailbox_notification}
        no_alert_for_skipped_runs: false
      permissions:
        - group_name: users
          level: CAN_MANAGE_RUN
      
      tasks:
        - task_key: task_start
          description: first task to install libraries and set base_parameters
          notebook_task:
            notebook_path: ../notebooks/task_start.py
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              azure_tenant_id: ${workspace.azure_tenant_id}
              azure_host: ${workspace.host}
              azure_client_id: ${workspace.azure_client_id}
              secret_scope: ${var.secret_scope}
          job_cluster_key: foundations_v1_job_crm_monthly_ghs_cluster${var.suffix} # Update to unique name for cluster as defined in job_clusters definition
          max_retries: 1
          min_retry_interval_millis: 600000
          retry_on_timeout: true
         
        - task_key: crm_monthly_ghs
          description: crm_monthly_ghs base data
          depends_on: 
          - task_key: task_start
          run_if: ALL_SUCCESS
          notebook_task:
            notebook_path: ../notebooks/crm_monthly_ghs.py
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              azure_tenant_id: ${workspace.azure_tenant_id}
              azure_host: ${workspace.host}
              azure_client_id: ${workspace.azure_client_id}
              secret_scope: ${var.secret_scope}
          job_cluster_key: foundations_v1_job_crm_monthly_ghs_cluster${var.suffix} # Update to unique name for cluster as defined in job_clusters definition
          max_retries: 1
          min_retry_interval_millis: 600000
          retry_on_timeout: true
          libraries:
            - pypi:
                package: databricks-sdk

        - task_key: task_detect_failure
          description: only when something failed
          depends_on: 
          - task_key: crm_monthly_ghs
          run_if: AT_LEAST_ONE_FAILED
          notebook_task:
            notebook_path: ../notebooks/task_detect_failure.py
          job_cluster_key: foundations_v1_job_crm_monthly_ghs_cluster${var.suffix} # Update to unique name for cluster as defined in job_clusters definition
          max_retries: 1
          min_retry_interval_millis: 600000
          retry_on_timeout: true
