# Databricks Jobs Configuration.
# This file defines the jobs and their configurations for deployment to Databricks.
# Each job can represent a notebook, library, or other executable entity within Databricks.
## for additional options please see schema.json in that repository (can be generated by "databricks bundle schema" command)
resources:
  jobs:
    foundations_v1_job_data_quality_master: # Provide a unique name for this bundle. This name should differentiate it from other projects.
      name: foundations_v1_job_data_quality_master${var.suffix} # Provide a unique name for this job. This name should differentiate it from other projects.
      job_clusters:
      # Configuration for the new cluster that will be spun up for this job.
      - job_cluster_key: foundations_v1_job_data_quality_master_cluster${var.suffix} # Provide a unique name for this cluster. This name should differentiate it from other projects.
        new_cluster: 
          num_workers: 8 # Number of worker nodes.
          driver_node_type_id: Standard_D8ds_v5 # use general type always for driver, monitor CPU and RAM usage of driver to adjust driver type, try to choose new generation of VMs
          node_type_id: Standard_D16ds_v5  # use compute optimized for ML tasks (Standard_F4s_v2, Standard_F8s_v2, Standard_F16s_v2), use general for data engineering, try to choose new generation of VMs
          spark_version: "14.3.x-photon-scala2.12"  # use ML LTS for ML tasks, use Photon LTS for data engineering
          azure_attributes:
            availability: "SPOT_WITH_FALLBACK_AZURE"
            spot_bid_max_price: -1
      max_concurrent_runs: 1
      timeout_seconds: 3400
      health:
        rules:
          - metric: RUN_DURATION_SECONDS
            op: GREATER_THAN
            value: 1800
      email_notifications:
        on_failure:
          - ${var.email_team_notification}
          - ${var.email_mailbox_notification}
        on_duration_warning_threshold_exceeded:
          - 68b2c6b0.asda.uk@uk.teams.ms
          - foundations-reply@asda.uk
        no_alert_for_skipped_runs: false
      schedule:
        quartz_cron_expression: "0 0,30 5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22 * * ?"
        timezone_id: Europe/London
        pause_status: ${var.cron_status}
      permissions:
        - group_name: users
          level: CAN_MANAGE_RUN
      
      tasks:
        - task_key: task_start
          description: first task to install libraries and set base_parameters
          notebook_task:
            notebook_path: ../notebooks/task_start.py
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              azure_tenant_id: ${workspace.azure_tenant_id}
              azure_host: ${workspace.host}
              azure_client_id: ${workspace.azure_client_id}
              secret_scope: ${var.secret_scope}
              concurrency_limit: "60"
              api_max_retries: "2"
              api_default_wait_time: "60"
          job_cluster_key: foundations_v1_job_data_quality_master_cluster${var.suffix} # Update to unique name for cluster as defined in job_clusters definition
          max_retries: 1
          min_retry_interval_millis: 300000
          retry_on_timeout: true
         
        - task_key: data_quality_master
          description: task to get run data quality checks
          depends_on: 
          - task_key: task_start
          run_if: ALL_SUCCESS
          notebook_task:
            notebook_path: ../notebooks/data_quality_master.py
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              azure_tenant_id: ${workspace.azure_tenant_id}
              azure_host: ${workspace.host}
              azure_client_id: ${workspace.azure_client_id}
              secret_scope: ${var.secret_scope}
              concurrency_limit: "60"
              api_max_retries: "2"
              api_default_wait_time: "60"
          job_cluster_key: foundations_v1_job_data_quality_master_cluster${var.suffix} # Update to unique name for cluster as defined in job_clusters definition
          max_retries: 2
          min_retry_interval_millis: 120000
          retry_on_timeout: true
          libraries:
            - pypi:
                package: databricks-sdk

        - task_key: data_quality_dependency_status
          description: task to load data quality table
          depends_on: 
          - task_key: data_quality_master
          run_if: ALL_SUCCESS
          notebook_task:
            notebook_path: ../data_quality_notebooks/data_quality_dependency_status.py
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
          job_cluster_key: foundations_v1_job_data_quality_master_cluster${var.suffix} # Update to unique name for cluster as defined in job_clusters definition
          max_retries: 2
          min_retry_interval_millis: 120000
          retry_on_timeout: true
          libraries:
            - pypi:
                package: databricks-sdk

        - task_key: data_quality_databricks_workflow_trigger
          description: task to trigger any databricks workflow dependencies
          depends_on: 
          - task_key: data_quality_dependency_status
          run_if: ALL_SUCCESS
          notebook_task:
            notebook_path: ../data_quality_notebooks/data_quality_databricks_workflow_trigger.py
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
          job_cluster_key: foundations_v1_job_data_quality_master_cluster${var.suffix} # Update to unique name for cluster as defined in job_clusters definition
          max_retries: 2
          min_retry_interval_millis: 120000
          retry_on_timeout: true
          libraries:
            - pypi:
                package: databricks-sdk

        - task_key: data_quality_power_bi_refresh_trigger
          description: task to trigger any power bi dependencies
          depends_on: 
          - task_key: data_quality_databricks_workflow_trigger
          run_if: ALL_SUCCESS
          notebook_task:
            notebook_path: ../data_quality_notebooks/data_quality_power_bi_refresh_trigger.py
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              pbi_spn_client_id: ${var.pbi_spn_client_id}
              pbi_spn_tenant_id: ${var.pbi_spn_tenant_id}
              secret_scope: ${var.secret_scope}
              api_max_retries: "2"
              api_default_wait_time: "60"
          job_cluster_key: foundations_v1_job_data_quality_master_cluster${var.suffix} # Update to unique name for cluster as defined in job_clusters definition
          max_retries: 2
          min_retry_interval_millis: 120000
          retry_on_timeout: true
          libraries:
            - pypi:
                package: databricks-sdk
        
        - task_key: data_quality_workflow_logging
          description: task to format presentation tables
          depends_on: 
          - task_key: data_quality_power_bi_refresh_trigger
          run_if: ALL_SUCCESS
          notebook_task:
            notebook_path: ../data_quality_notebooks/data_quality_workflow_logging.py
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              pbi_spn_client_id: ${var.pbi_spn_client_id}
              pbi_spn_tenant_id: ${var.pbi_spn_tenant_id}
              secret_scope: ${var.secret_scope}
              api_max_retries: "2"
              api_default_wait_time: "60"
          job_cluster_key: foundations_v1_job_data_quality_master_cluster${var.suffix} # Update to unique name for cluster as defined in job_clusters definition
          max_retries: 2
          min_retry_interval_millis: 120000
          retry_on_timeout: true
          libraries:
            - pypi:
                package: databricks-sdk

        - task_key: refresh_data_quality_report
          description: refresh the data quality PBI
          depends_on: 
          - task_key: data_quality_workflow_logging
          run_if: ALL_SUCCESS
          notebook_task:
            notebook_path: ../data_quality_notebooks/refresh_data_quality_report.py
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              pbi_spn_client_id: ${var.pbi_spn_client_id}
              pbi_spn_tenant_id: ${var.pbi_spn_tenant_id}
              secret_scope: ${var.secret_scope}
              api_max_retries: "2"
              api_default_wait_time: "60"
          job_cluster_key: foundations_v1_job_data_quality_master_cluster${var.suffix} # Update to unique name for cluster as defined in job_clusters definition
          max_retries: 2
          min_retry_interval_millis: 120000
          retry_on_timeout: true
          libraries:
            - pypi:
                package: databricks-sdk

        - task_key: task_detect_failure
          description: only when something failed
          depends_on: 
          - task_key: data_quality_master
          - task_key: data_quality_dependency_status
          - task_key: data_quality_databricks_workflow_trigger
          - task_key: data_quality_power_bi_refresh_trigger
          - task_key: data_quality_workflow_logging
          - task_key: refresh_data_quality_report
          run_if: AT_LEAST_ONE_FAILED
          notebook_task:
            notebook_path: ../notebooks/task_detect_failure.py
          job_cluster_key: foundations_v1_job_data_quality_master_cluster${var.suffix} # Update to unique name for cluster as defined in job_clusters definition
          max_retries: 0
          min_retry_interval_millis: 600000
          retry_on_timeout: true
