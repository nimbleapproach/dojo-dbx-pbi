# Databricks Jobs Configuration.
# This file defines the jobs and their configurations for deployment to Databricks.
# Each job can represent a notebook, library, or other executable entity within Databricks.
## for additional options please see schema.json in that repository (can be generated by "databricks bundle schema" command)
resources:
  jobs:
    foundations_v1_job_deploy_ddl: # Provide a unique name for this bundle. This name should differentiate it from other projects.
      name: foundations_v1_job_deploy_ddl${var.suffix} # Provide a unique name for this job. This name should differentiate it from other projects.
      job_clusters:
      # Configuration for the new cluster that will be spun up for this job.
      - job_cluster_key: foundations_v1_job_deploy_dll_cluster${var.suffix} # Provide a unique name for this cluster. This name should differentiate it from other projects.
        new_cluster: 
          num_workers: 16 # Number of worker nodes.
          driver_node_type_id: Standard_D4ds_v5 # use general type always for driver, monitor CPU and RAM usage of driver to adjust driver type, try to choose new generation of VMs
          node_type_id: Standard_D16ds_v4 # use compute optimized for ML tasks (Standard_F4s_v2, Standard_F8s_v2, Standard_F16s_v2), use general for data engineering, try to choose new generation of VMs
          spark_version: "14.3.x-photon-scala2.12" # use ML LTS for ML tasks, use Photon LTS for data engineering
          azure_attributes:
            availability: "SPOT_WITH_FALLBACK_AZURE"
            spot_bid_max_price: -1
      max_concurrent_runs: 1
      timeout_seconds: 14400
      email_notifications:
        on_failure:
          - ${var.email_team_notification}
          - ${var.email_mailbox_notification}
        no_alert_for_skipped_runs: false
      permissions:
        - group_name: users
          level: CAN_MANAGE_RUN
      
      tasks:
        - task_key: deploy_ddl
          description: task to deploy DDLs 
          notebook_task:
            notebook_path: ../notebooks/deploy_ddl.py
            base_parameters:
              catalog: ${var.catalog}
              schema: ${var.schema}
              external_volume_location_name: ${var.external_volume_location_name}
              external_volume_location_path: ${var.external_volume_location_path}
              azure_tenant_id: ${workspace.azure_tenant_id}
              azure_host: ${workspace.host}
              azure_client_id: ${workspace.azure_client_id}
              secret_scope: ${var.secret_scope}
          job_cluster_key: foundations_v1_job_deploy_dll_cluster${var.suffix} # Update to unique name for cluster as defined in job_clusters definition
          max_retries: 1
          min_retry_interval_millis: 100000
          retry_on_timeout: true
          libraries:
            - pypi:
                package: fsspec
          
        - task_key: run_unit_tests
          description: run all unit tests
          depends_on: 
            - task_key: deploy_ddl
          notebook_task:
            notebook_path: ../notebooks/run_unit_tests.py
          job_cluster_key: foundations_v1_job_deploy_dll_cluster${var.suffix} # Update to unique name for cluster as defined in job_clusters definition
          max_retries: 1
          min_retry_interval_millis: 100000
          retry_on_timeout: False
          libraries:
            - pypi:
                package: databricks-sdk
            - pypi:
                package: pytest